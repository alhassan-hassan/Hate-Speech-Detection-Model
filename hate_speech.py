# -*- coding: utf-8 -*-
"""NLP_Model_Hate-Speech-Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oUBhV7-iXOfb7JILkYfB9b-mcKM3Iqxk
"""

!pip install datasets
!pip install gradio

import gradio as gr
import pandas as pd
import nltk
from nltk.util import pr
import pandas as pd
import numpy as np
from keras import models
from keras import layers
from keras import regularizers
from keras.layers import LSTM
from tensorflow.keras.layers import Dense
from keras.regularizers import l2
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from keras import backend as K
import string
import re
from nltk.corpus import stopwords
import datasets
from sklearn.model_selection import train_test_split
from keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from keras.callbacks import EarlyStopping, ReduceLROnPlateau


nltk.download('stopwords')
stemmer = nltk.SnowballStemmer("english")
stopword=set(stopwords.words('english'))
nltk.download('stopwords')
nltk.download('punkt')

# HYPERPARAMETERS
LEARNING_RATE   = 0.005
BATCH_SIZE      = 32
NUM_EPOCHS      = 15
embedding_dim = 16   #This is a multiplier to the vocab to make a matrix for the texts
vocab_size = 30226   #This means a set of possible unique words that the model can learn

# OTHER PARAMETERS
training_size = 0.60
testing_size = 0.4801

# DOWNLOADING THE DATASET
dataset = datasets.load_dataset('ucberkeley-dlab/measuring-hate-speech', 'default')
# dataset = dataset['train'].to_pandas()

# FISHING OUT THE RELEVANT INFORMATION IN THE MODEL TRAINING
text = list()
score = list()
label = list ()

# ITERATE AND GET EACH DATA INTO THE DIFFERENT LIST OBJECTS
for row in range(len(dataset['train'])):
  if row == 50000:
    break
  text.append(dataset['train'][row]['text'])
  score.append(dataset['train'][row]['hate_speech_score'])
  if dataset['train'][row]['hate_speech_score'] > 0:
    label.append(1)
  else:
    label.append(0)

# Note: Each function processes one text at a time.
stop_words = set(stopwords.words('english'))
# Adding "rt" to remove retweets from the dataset (considered as noise).
stop_words.add("rt")

# THIS FUNCTION DEEP CLEANS THE DATASET BASED ON THE PATTERN OBSERVED IN THE DATASET
def clean(text):
    # make all words lowercase
    text = str(text).lower()

    # remove any leading and training whitespaces
    text = text.strip()

    # remove emphasies, symbols and character leading words example @wjdsd should be removed
    text = re.sub(r'@[A-Za-z0-9]+', '', text)
    text = re.sub('\[.*?\]', '', text)
    text = re.sub('#', '', text)
    text = re.sub('https?://\S+|www\.\S+', '', text)  # remove hyperlinks also
    text = re.sub('<.*?>+', '', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)

    # remove new lines
    text = re.sub('\n', '', text)
    text = re.sub('\w*\d\w*', '', text)
    text = [word for word in text.split(' ') if word not in stopword]
    text=" ".join(text)
    text = [stemmer.stem(word) for word in text.split(' ')]
    text=" ".join(text)
    return text
print(stop_words)

# Sample stopwords list
stopwords_list = [
    "the", "and", "of", "to", "a", "in",
    "is", "it", "that", "you", "for", "this",
    "with", "on", "be", "as", "are", "not",
    "or", "but", "by", "at", "from", "which",
    "will", "have", "has", "was", "can", "do",
    "an", "if", "their", "they", "what", "all",
    "there", "we", "your", "who", "when", "where",
    "how", "why", "would", "should", "could", "into",
    "out", "up", "down", "over", "under", "between",
    "before", "after", "above", "below", "between", "through",
    "during", "since", "until", "among", "against", "along"
]

# Chunk the stopwords list into rows with six words each
chunked_stopwords = [stopwords_list[i:i + 6] for i in range(0, len(stopwords_list), 6)]

# Display the stopwords in a tabular format
table = tabulate(chunked_stopwords, headers=["Stopword 1", "Stopword 2", "Stopword 3", "Stopword 4", "Stopword 5", "Stopword 6"], tablefmt="grid")
print(table)

from tabulate import tabulate

cleaned_tweets = [[tweet, clean(tweet)] for tweet in text[0:100] if len(tweet) < 50]

table = tabulate(cleaned_tweets, headers=["Original Tweet", "Preprocessed Tweet"], tablefmt="fancy_grid")

print(table)

tweets = [clean(tweet) for tweet in text]
X_train, X_test, y_train, y_test = train_test_split(tweets, label, test_size=0.2, random_state=42)

## Tokenizing -> basically we use tokenisation for many things, its commonly used for feature extraction in preprocessing. btw idk how it works as feature extraction tho :(
# declare the tokenizer
tokenizer = Tokenizer()
# build the vocabulary based on train dataset
tokenizer.fit_on_texts(X_train)
# tokenize the train and test dataset
X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)

# vocabulary size (num of unique words) -> will be used in embedding layer
vocab_size = len(tokenizer.word_index) + 1

## Padding -> to uniform the datas
max_length = max(len(seq) for seq in X_train)

# to test an outlier case (if one of the test dataset has longer length)
for x in X_test:
    if len(x) > max_length:
        print(f"an outlier detected: {x}")

X_train = pad_sequences(X_train, maxlen = max_length)
X_test = pad_sequences(X_test, maxlen = max_length)

y_train = np.array(y_train)
y_test = np.array(y_test)

print(y_test)

print(f"num test tweet: {y_test.shape}")
print(f"num train tweet: {y_train.shape}")

import keras.backend as K

def recall(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

def precision(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision

def f1(y_true, y_pred):
    precisions = precision(y_true, y_pred)
    recalls = recall(y_true, y_pred)
    f1 = 2*((precisions*recalls)/(precisions+recalls+K.epsilon()))
    return f1

from keras.callbacks import EarlyStopping, ReduceLROnPlateau

# Define callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-6)

# Define the model
model = Sequential([
    # embedding layer
    Embedding(vocab_size, 200, input_length=max_length),

    # LSTM layer
    LSTM(32, dropout=0.2, recurrent_dropout=0.2),

    # dropout layer
    Dropout(0.3),

    # dense layer
    Dense(64, activation="relu"),

    # dropout layer
    Dropout(0.3),

    # output layer with sigmoid activation for binary classification
    Dense(1, activation="sigmoid"),
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', f1, precision, recall])

# Train the model with callbacks
model_history = model.fit(
    X_train,
    y_train,
    batch_size=32,  # Reduced batch size
    epochs=10,
    validation_data=(X_test, y_test),
    callbacks=[early_stopping, reduce_lr]
)

model.summary()

# checking the model parameters
model.summary()

import matplotlib.pyplot as plt
hist = model_history.history
plt.plot(hist['loss'],'r',linewidth=2, label='Training loss')
plt.plot(hist['val_loss'], 'g',linewidth=2, label='Validation loss')
plt.title('Hate Speech and Offensive language Model')
plt.xlabel('Epochs numbers')
plt.ylabel('MSE numbers')
plt.show()

def predict_tweet(text):
    orinal_text = text
    # Preprocess the text
    text = clean(text)

    # Convert the preprocessed text to sequence
    text_sequence = tokenizer.texts_to_sequences([text])

    # Pad the sequence to the same length as training data
    text_sequence = pad_sequences(text_sequence, maxlen=max_length)

    # Predict the probabilities for each class for the text
    probabilities = model.predict(text_sequence)
    print(probabilities)

    # Get the predicted class label (class with the highest probability)
    predicted_class = np.argmax(probabilities, axis=1)[0]

    # Return the prediction result
    if probabilities > 0.5:
        return f"{orinal_text} is a hate-speech."
    else:
        return f"{orinal_text} is NOT a hate speech."

iface = gr.Interface(fn=predict_tweet, inputs="text", outputs="text")
iface.launch()
